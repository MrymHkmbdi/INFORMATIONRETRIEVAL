{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IRM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrymHkmbdi/TFIDF_COSINESIMILARITY_INFORMATIONRETRIEVAL/blob/main/IRM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6j5Fjut2Yel1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deb5c296-ad5e-4aa9-840c-f4edef14c37d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQLZbF-0pG0w"
      },
      "source": [
        "# Library Installation and Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEFY07kMP8Ra"
      },
      "source": [
        "pip install clean-text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yL8aHLj3RFZ8"
      },
      "source": [
        "pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOA4SWszRQsc"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVSF1Wbdpuvj"
      },
      "source": [
        "## Phase 1 Libariries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZbU-H0BPVow",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "756c4e0c-5280-4707-9443-8983019607d3"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from cleantext import clean\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD_WOxEmp7GB"
      },
      "source": [
        "## Phase 2 Libariries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAnzwiDXpqSu"
      },
      "source": [
        "from pathlib import Path\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQKM8yJxs9Js"
      },
      "source": [
        "# Phase 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygm3PPfkQ1Z4"
      },
      "source": [
        "#### in this phase the data read from plot_summaries.txt were converted to csv and cleaned.\n",
        "#### After that feature of searching were added. in this part when you search a term, number o repeats and ids of movies containing the term will be shown."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXVVKiCoRuFs"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/plot_summaries.txt', delimiter='\\t', names=['id', 'overview'])\n",
        "# dataframe = df.to_csv('plot.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "D-xBssO_SpKC",
        "outputId": "9f554cdf-af09-4666-d8d4-45ea3517f339"
      },
      "source": [
        "ps = PorterStemmer()\n",
        "cleans_data=[]\n",
        "for i in range(0,len(data)):\n",
        "  clean_data=clean(data.loc[i,\"overview\"],\n",
        "        fix_unicode=True,               # fix various unicode errors\n",
        "        to_ascii=True,                  # transliterate to closest ASCII representation\n",
        "        lower=True,                     # lowercase text\n",
        "        no_line_breaks=True,           # fully strip line breaks as opposed to only normalizing them\n",
        "        no_urls=True,                  # replace all URLs with a special token\n",
        "        no_emails=True,                # replace all email addresses with a special token\n",
        "        no_phone_numbers=True,         # replace all phone numbers with a special token\n",
        "        no_numbers=False,               # replace all numbers with a special token\n",
        "        no_digits=False,                # replace all digits with a special token\n",
        "        no_currency_symbols=True,      # replace all currency symbols with a special token\n",
        "        no_punct=True,                 # remove punctuations\n",
        "        replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n",
        "        replace_with_url=\"<URL>\",\n",
        "        replace_with_email=\"<EMAIL>\",\n",
        "        replace_with_phone_number=\"<PHONE>\",\n",
        "        replace_with_number=\"<NUMBER>\",\n",
        "        replace_with_digit=\"0\",\n",
        "        replace_with_currency_symbol=\"<CUR>\",\n",
        "        lang=\"en\"                       # set to 'de' for German special handling\n",
        "    )\n",
        "  nltk_english_stopwords = stopwords.words('english')\n",
        "  clean_data_final = \"\"\n",
        "  words=re.sub(\"[^\\w]\", \" \", clean_data).split()\n",
        "  for w in words:\n",
        "    if w not in nltk_english_stopwords:\n",
        "      clean_data_final = clean_data_final + \" \"+ ps.stem(w)\n",
        "  data.loc[i,\"cleaned_data\"] = clean_data_final"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-a9ec3aae6d75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcleans_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   clean_data=clean(data.loc[i,\"overview\"],\n\u001b[1;32m      5\u001b[0m         \u001b[0mfix_unicode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m               \u001b[0;31m# fix various unicode errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1AarLldXFH8"
      },
      "source": [
        "data.to_csv('plot_cleaned.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c8FWOvDX-B4"
      },
      "source": [
        "cleaned_data_file = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/plot_cleaned.csv',encoding='utf-8')\n",
        "ps = PorterStemmer()\n",
        "cleans_data=[]\n",
        "terms = []\n",
        "counter = 0\n",
        "input_term = input()\n",
        "clean_data=clean('{}'.format(input_term),\n",
        "        fix_unicode=True,               # fix various unicode errors\n",
        "        to_ascii=True,                  # transliterate to closest ASCII representation\n",
        "        lower=True,                     # lowercase text\n",
        "        no_line_breaks=True,           # fully strip line breaks as opposed to only normalizing them\n",
        "        no_urls=True,                  # replace all URLs with a special token\n",
        "        no_emails=True,                # replace all email addresses with a special token\n",
        "        no_phone_numbers=True,         # replace all phone numbers with a special token\n",
        "        no_numbers=False,               # replace all numbers with a special token\n",
        "        no_digits=False,                # replace all digits with a special token\n",
        "        no_currency_symbols=True,      # replace all currency symbols with a special token\n",
        "        no_punct=True,                 # remove punctuations\n",
        "        replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n",
        "        replace_with_url=\"<URL>\",\n",
        "        replace_with_email=\"<EMAIL>\",\n",
        "        replace_with_phone_number=\"<PHONE>\",\n",
        "        replace_with_number=\"<NUMBER>\",\n",
        "        replace_with_digit=\"0\",\n",
        "        replace_with_currency_symbol=\"<CUR>\",\n",
        "        lang=\"en\"                       # set to 'de' for German special handling\n",
        "    )\n",
        "ans = ps.stem(clean_data)\n",
        "for i in range(0,len(cleaned_data_file)):\n",
        "  for word in cleaned_data_file.loc[i, \"cleaned_data\"].split():\n",
        "    if ans in word:\n",
        "      terms.append(cleaned_data_file.loc[i, \"id\"])\n",
        "    else:\n",
        "      continue\n",
        "mylist = list(dict.fromkeys(terms))\n",
        "print('Term {} repeated {} times in films listed below: '.format(input_term, len(mylist)))\n",
        "print(mylist)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMZBNjWRbzYp"
      },
      "source": [
        "cleaned_data_file = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/plot_cleaned.csv',encoding='utf-8')\n",
        "inv_idx_dict = {}\n",
        "for i in range(0, len(clened_data_file)):\n",
        "  for word in cleaned_data_file.loc[i, \"cleaned_data\"].split():\n",
        "    if word not in inv_idx_dict.keys():\n",
        "      inv_idx_dict[word] = [cleaned_data_file.loc[i, \"id\"]]\n",
        "    else:\n",
        "      inv_idx_dict[word].append(cleaned_data_file.loc[i, \"id\"])\n",
        "for each_key, value in inv_idx_dict.items():\n",
        "   inv_idx_dict[each_key] = list(set(value))\n",
        "print(inv_idx_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3yhjJrEcbf4"
      },
      "source": [
        "(pd.DataFrame.from_dict(data=inv_idx_dict, orient='index')\n",
        "   .to_csv('/content/drive/MyDrive/Colab Notebooks/inverted-index.csv', header=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59BmevGXpSxq"
      },
      "source": [
        "# TF-IDF AND COSINE SIMILARITY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2skf68MeR-Jo"
      },
      "source": [
        "#### in this phase calculating the cosine similarity between whole document and a query was added. the result was a list of 10 most related movies to our query."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9CwJ0cvpsHa"
      },
      "source": [
        "plot_cleaned_file = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/irm-hw1/plot_cleaned.csv',encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vOe87D1qu12"
      },
      "source": [
        "query = input()\n",
        "ps = PorterStemmer()\n",
        "cleans_data=[]\n",
        "clean_data=clean('{}'.format(query),\n",
        "        fix_unicode=True,               # fix various unicode errors\n",
        "        to_ascii=True,                  # transliterate to closest ASCII representation\n",
        "        lower=True,                     # lowercase text\n",
        "        no_line_breaks=True,           # fully strip line breaks as opposed to only normalizing them\n",
        "        no_urls=True,                  # replace all URLs with a special token\n",
        "        no_emails=True,                # replace all email addresses with a special token\n",
        "        no_phone_numbers=True,         # replace all phone numbers with a special token\n",
        "        no_numbers=False,               # replace all numbers with a special token\n",
        "        no_digits=False,                # replace all digits with a special token\n",
        "        no_currency_symbols=True,      # replace all currency symbols with a special token\n",
        "        no_punct=True,                 # remove punctuations\n",
        "        replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n",
        "        replace_with_url=\"<URL>\",\n",
        "        replace_with_email=\"<EMAIL>\",\n",
        "        replace_with_phone_number=\"<PHONE>\",\n",
        "        replace_with_number=\"<NUMBER>\",\n",
        "        replace_with_digit=\"0\",\n",
        "        replace_with_currency_symbol=\"<CUR>\",\n",
        "        lang=\"en\"                       # set to 'de' for German special handling\n",
        "  )\n",
        "nltk_english_stopwords = stopwords.words('english')\n",
        "cleaned_query = \"\"\n",
        "words=re.sub(\"[^\\w]\", \" \", clean_data).split()\n",
        "for w in words:\n",
        "  if w not in nltk_english_stopwords:\n",
        "    cleaned_query = cleaned_query + \" \"+ ps.stem(w)\n",
        "print(cleaned_query)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmFlf2pn1faJ"
      },
      "source": [
        "tfidf_score = TfidfVectorizer()\n",
        "movie_tfidf = tfidf_score.fit_transform(plot_cleaned_file['cleaned_data'].to_list())\n",
        "query_tfidf = tfidf_score.transform([cleaned_query])\n",
        "similarity = cosine_similarity(query_tfidf, movie_tfidf)\n",
        "for i, row in enumerate(similarity):\n",
        "    plot_cleaned_file.loc[(-row).argsort()[:10]].to_csv(\"queryandmovies/{}.csv\".format(query),index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVKIsuOc5ZpG"
      },
      "source": [
        "queriy_and_movies = pd.read_csv(\"/content/drive/MyDrive/queryandmovies/1.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdaNDJcJIBMy"
      },
      "source": [
        "ids = []\n",
        "overview = []\n",
        "for i in range(len(queriy_and_movies)):\n",
        "  overview.append(queriy_and_movies.loc[i, 'overview'])\n",
        "  ids.append(queriy_and_movies.loc[i, 'id'])\n",
        "zip_it = zip(ids, overview)\n",
        "id_plot_dict = dict(zip_it)\n",
        "for k,v in id_plot_dict.items():\n",
        "  print('{}: {}\\n'.format(k,v))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdbtqopkIEjE"
      },
      "source": [
        "tfidf_score = TfidfVectorizer()\n",
        "plot = [plot_cleaned_file.loc[1, 'cleaned_data']]\n",
        "tf_idf =  tfidf_score.fit_transform(plot)\n",
        "df = pd.DataFrame(tf_idf.toarray(), columns=tfidf_score.get_feature_names())\n",
        "df.to_csv('tfidf_table.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}